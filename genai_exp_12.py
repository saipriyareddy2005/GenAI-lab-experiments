# -*- coding: utf-8 -*-
"""genAI_exp_12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JiEJwMsXao4r-7vR8PEKdLtTVVT3y4YJ
"""

# import gym # Replaced by gymnasium
import gymnasium as gym
import numpy as np
import random

# --- Step 1: Initialize Environment ---
# env = gym.make("Taxi-v3") # Replaced by gymnasium and new step api
env = gym.make("Taxi-v3")
state_space = env.observation_space.n
action_space = env.action_space.n

# --- Step 2: Initialize Q-table ---
Q = np.zeros((state_space, action_space))

# --- Step 3: Hyperparameters ---
alpha = 0.7        # Learning rate
gamma = 0.9        # Discount factor
epsilon = 1.0      # Exploration rate
epsilon_decay = 0.01
episodes = 1000
max_steps = 100

# --- Step 4: Q-Learning Training Loop ---
for episode in range(episodes):
    # state = env.reset()[0] # Corrected to handle tuple return from reset()
    state, info = env.reset()
    done = False

    for _ in range(max_steps):
        # Îµ-greedy action selection
        if random.uniform(0, 1) < epsilon:
            action = env.action_space.sample()
        else:
            action = np.argmax(Q[state, :])

        # next_state, reward, done, truncated, info = env.step(action) # Corrected to handle tuple return from step()
        next_state, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        # Q-Learning update rule
        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])

        state = next_state
        if done:
            break

    # Decay epsilon
    epsilon = max(0.01, epsilon - epsilon_decay)

print("Training completed!")

# --- Step 5: Evaluate the Agent ---
total_rewards = 0
episodes_test = 100
for _ in range(episodes_test):
    # state = env.reset()[0] # Corrected to handle tuple return from reset()
    state, info = env.reset()
    done = False
    episode_reward = 0

    while not done:
        action = np.argmax(Q[state, :])
        # state, reward, done, truncated, info = env.step(action) # Corrected to handle tuple return from step()
        state, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        episode_reward += reward

    total_rewards += episode_reward

print("Average reward over 100 test episodes:", total_rewards / episodes_test)

# --- Step 6: LLM Integration (Example Stub) ---
def explain_policy(state):
    """Simulate LLM explanation for the policy."""
    best_action = np.argmax(Q[state])
    action_names = ["South", "North", "East", "West", "Pickup", "Dropoff"]
    explanation = f"In state {state}, the best action is '{action_names[best_action]}'."
    explanation += " This action was chosen because it leads to a higher long-term reward."
    return explanation

# Test LLM-style explanation
sample_state = random.randint(0, state_space - 1)
print(explain_policy(sample_state))

